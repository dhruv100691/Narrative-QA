{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Implementation of Pointer networks: http://arxiv.org/pdf/1506.03134v1.pdf.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import DataGenerator\n",
    "from pointer import pointer_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('batch_size', 32, 'Batch size.  ')\n",
    "flags.DEFINE_integer('max_steps', 10, 'Number of numbers to sort.  ')\n",
    "flags.DEFINE_integer('rnn_size', 32, 'RNN size.  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointerNetwork(object):\n",
    "    \n",
    "    def __init__(self, max_len, input_size, size, num_layers, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor):\n",
    "        \"\"\"Create the network. A simplified network that handles only sorting.\n",
    "        \n",
    "        Args:\n",
    "            max_len: maximum length of the model.\n",
    "            input_size: size of the inputs data.\n",
    "            size: number of units in each layer of the model.\n",
    "            num_layers: number of layers in the model.\n",
    "            max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "            batch_size: the size of the batches used during training;\n",
    "                the model construction is independent of batch_size, so it can be\n",
    "                changed after initialization if this is convenient, e.g., for decoding.\n",
    "            learning_rate: learning rate to start with.\n",
    "            learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "            \n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.decoder_targets = []\n",
    "        self.target_weights = []\n",
    "        for i in range(max_len):\n",
    "            self.encoder_inputs.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, input_size], name=\"EncoderInput%d\" % i))\n",
    "\n",
    "        for i in range(max_len + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, input_size], name=\"DecoderInput%d\" % i))\n",
    "            self.decoder_targets.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, max_len + 1], name=\"DecoderTarget%d\" % i))  # one hot\n",
    "            self.target_weights.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, 1], name=\"TargetWeight%d\" % i))\n",
    "\n",
    "            \n",
    "        # Encoder\n",
    "        \n",
    "        # Need for attention\n",
    "        encoder_outputs, final_state = tf.nn.rnn(cell, self.encoder_inputs, dtype = tf.float32)\n",
    "        \n",
    "        # Need a dummy output to point on it. End of decoding.\n",
    "        encoder_outputs = [tf.zeros([FLAGS.batch_size, FLAGS.rnn_size])] + encoder_outputs\n",
    "\n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        top_states = [tf.reshape(e, [-1, 1, cell.output_size])\n",
    "                      for e in encoder_outputs]\n",
    "        attention_states = tf.concat(1, top_states)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            outputs, states, _ = pointer_decoder(\n",
    "                self.decoder_inputs, final_state, attention_states, cell)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\", reuse=True):\n",
    "            predictions, _, inps = pointer_decoder(\n",
    "                self.decoder_inputs, final_state, attention_states, cell, feed_prev=True)\n",
    "            \n",
    "        self.predictions = predictions\n",
    "\n",
    "        self.outputs = outputs\n",
    "        self.inps = inps\n",
    "        # move code below to a separate function as in TF examples\n",
    "        \n",
    "            \n",
    "    def create_feed_dict(self, encoder_input_data, decoder_input_data, decoder_target_data):\n",
    "        feed_dict = {}\n",
    "        for placeholder, data in zip(self.encoder_inputs, encoder_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder, data in zip(self.decoder_inputs, decoder_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder, data in zip(self.decoder_targets, decoder_target_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder in self.target_weights:\n",
    "            feed_dict[placeholder] = np.ones([self.batch_size, 1])\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        loss = 0.0\n",
    "        for output, target, weight in zip(self.outputs, self.decoder_targets, self.target_weights):\n",
    "            loss += tf.nn.softmax_cross_entropy_with_logits(output, target) * weight\n",
    "\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        test_loss = 0.0\n",
    "        for output, target, weight in zip(self.predictions, self.decoder_targets, self.target_weights):\n",
    "            test_loss += tf.nn.softmax_cross_entropy_with_logits(output, target) * weight\n",
    "\n",
    "        test_loss = tf.reduce_mean(test_loss)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        train_loss_value = 0.0\n",
    "        test_loss_value = 0.0\n",
    "        \n",
    "        correct_order = 0\n",
    "        all_order = 0\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            merged = tf.merge_all_summaries()\n",
    "            writer = tf.train.SummaryWriter(\"/tmp/pointer_logs\", sess.graph)\n",
    "            init = tf.initialize_all_variables()\n",
    "            sess.run(init)\n",
    "            for i in range(10000):\n",
    "                encoder_input_data, decoder_input_data, targets_data = dataset.next_batch(\n",
    "                    FLAGS.batch_size, FLAGS.max_steps)\n",
    "\n",
    "                # Train\n",
    "                feed_dict = self.create_feed_dict(\n",
    "                    encoder_input_data, decoder_input_data, targets_data)\n",
    "                d_x, l = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                train_loss_value = 0.9 * train_loss_value + 0.1 * d_x\n",
    "                                \n",
    "                if i % 100 == 0:\n",
    "                    print('Step: %d' % i)\n",
    "                    print(\"Train: \", train_loss_value)\n",
    "\n",
    "                encoder_input_data, decoder_input_data, targets_data = dataset.next_batch(\n",
    "                    FLAGS.batch_size, FLAGS.max_steps, train_mode=False)\n",
    "                # Test\n",
    "                feed_dict = self.create_feed_dict(\n",
    "                    encoder_input_data, decoder_input_data, targets_data)\n",
    "                inps_ = sess.run(self.inps, feed_dict=feed_dict)\n",
    "\n",
    "                predictions = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "                \n",
    "                test_loss_value = 0.9 * test_loss_value + 0.1 * sess.run(test_loss, feed_dict=feed_dict)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Test: \", test_loss_value)\n",
    "\n",
    "                predictions_order = np.concatenate([np.expand_dims(prediction , 0) for prediction in predictions])\n",
    "                predictions_order = np.argmax(predictions_order, 2).transpose(1, 0)[:,0:FLAGS.max_steps]\n",
    "                    \n",
    "                input_order = np.concatenate([np.expand_dims(encoder_input_data_ , 0) for encoder_input_data_ in encoder_input_data])\n",
    "                input_order = np.argsort(input_order, 0).squeeze().transpose(1, 0)+1\n",
    "                \n",
    "                correct_order += np.sum(np.all(predictions_order == input_order,\n",
    "                                    axis=1))\n",
    "                all_order += FLAGS.batch_size\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print('Correct order / All order: %f' % (correct_order / all_order))\n",
    "                    correct_order = 0\n",
    "                    all_order = 0\n",
    "                    \n",
    "                    # print(encoder_input_data, decoder_input_data, targets_data)\n",
    "                    # print(inps_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pointer_network = PointerNetwork(FLAGS.max_steps, 1, FLAGS.rnn_size, 1, 5, FLAGS.batch_size, 1e-2, 0.95)\n",
    "dataset = DataGenerator()\n",
    "pointer_network.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
